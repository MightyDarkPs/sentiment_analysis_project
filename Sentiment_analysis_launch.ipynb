{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "01_822YZSMHv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4NfjRzUSOaW",
        "outputId": "9e90e916-4ea5-4f30-ebd1-6eeb1e651967"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-Wi3Ulhev6E",
        "outputId": "bcab7e89-04b2-4195-9487-5c6050fa6579"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your tweet: i love twitter\n",
            "             text sentiment\n",
            "0  i love twitter  Positive\n",
            "Enter your tweet: i hate to do advanced programming homework\n",
            "                                         text sentiment\n",
            "0                              i love twitter  Positive\n",
            "1  i hate to do advanced programming homework  Negative\n",
            "Enter your tweet: thank you\n",
            "                                         text sentiment\n",
            "0                              i love twitter  Positive\n",
            "1  i hate to do advanced programming homework  Negative\n",
            "2                                   thank you  Positive\n",
            "Enter your tweet: good luck kazakhstan\n",
            "                                         text sentiment\n",
            "0                              i love twitter  Positive\n",
            "1  i hate to do advanced programming homework  Negative\n",
            "2                                   thank you  Positive\n",
            "3                        good luck kazakhstan  Positive\n"
          ]
        }
      ],
      "source": [
        "def predict(vectoriser, model, text):\n",
        "    processedText = []\n",
        "\n",
        "    wordLemm = WordNetLemmatizer()\n",
        "\n",
        "    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n",
        "    userPattern       = '@[^\\s]+'\n",
        "    alphaPattern      = \"[^a-zA-Z0-9]\"\n",
        "    sequencePattern   = r\"(.)\\1\\1+\"\n",
        "    seqReplacePattern = r\"\\1\\1\"\n",
        "\n",
        "    for tweet in text:\n",
        "        tweet = tweet.lower()\n",
        "\n",
        "        tweet = re.sub(urlPattern,' URL',tweet)\n",
        "        for emoji in emojis.keys():\n",
        "            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])\n",
        "        tweet = re.sub(userPattern,' USER', tweet)\n",
        "        tweet = re.sub(alphaPattern, \" \", tweet)\n",
        "        tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n",
        "\n",
        "        tweetwords = ''\n",
        "        for word in tweet.split():\n",
        "            if len(word)>1:\n",
        "                word = wordLemm.lemmatize(word)\n",
        "                tweetwords += (word+' ')\n",
        "\n",
        "        processedText.append(tweetwords)\n",
        "\n",
        "    textdata = vectoriser.transform(processedText)\n",
        "    sentiment = model.predict(textdata)\n",
        "\n",
        "    data = []\n",
        "    for text, pred in zip(text, sentiment):\n",
        "        data.append((text,pred))\n",
        "\n",
        "    df = pd.DataFrame(data, columns = ['text','sentiment'])\n",
        "    df = df.replace([0,1], [\"Negative\",\"Positive\"])\n",
        "    return df\n",
        "\n",
        "emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad',\n",
        "          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n",
        "          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed',\n",
        "          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n",
        "          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n",
        "          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink',\n",
        "          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n",
        "\n",
        "file = open('/content/vectoriser.pickle', 'rb')\n",
        "vectoriser = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "file = open('/content/model.pickle', 'rb')\n",
        "model = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "text = []\n",
        "\n",
        "while (True):\n",
        "  var = input(\"Enter your tweet: \")\n",
        "  text.append(var)\n",
        "  df = predict(vectoriser, model, text)\n",
        "  print(df)  \n"
      ]
    }
  ]
}